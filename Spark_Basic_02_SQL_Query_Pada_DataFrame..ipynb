{
  "metadata": {
    "name": "Spark_Basic_02_SQL_Query_Pada_DataFrame",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "SQL adalah salah satu bahasa populer untuk pemrosesan dan analisis data. Spark mendukung SQL untuk memproses DataFrame. Dalam latihan ini kita akan menggunakan spark SQL tanpa database."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Inisialisasi spark session untuk berinteraksi dengan Spark cluster."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder.appName(\u0027DataFrame Basics\u0027).getOrCreate()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Download Dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget https://github.com/urfie/SparkSQL-dengan-Hive/raw/main/datasets/application_record_header.csv.gz"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Load ke Dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -mkdir /user/userdev/csv_dataframe\nhdfs dfs -put /home/userdev/application_record_header.csv.gz /user/userdev/csv_dataframe\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/userdev/csv_dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d spark.read.csv(\"/user/userdev/csv_dataframe/application_record_header.csv.gz\", header\u003dTrue, inferSchema\u003dTrue)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Sebelum menggunakan SQL, kita perlu membuat temporary table dari dataframe yang akan kita olah.\n\nGunakan fungsi `createOrReplaceTempView(nama_tabel)` pada dataframe tersebut."
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.createOrReplaceTempView(\"app_record\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Selanjutnya kita bisa menggunakan nama tabel yang sudah kita definisikan dalam SQL statement.\n\nUntuk mengeksekusi SQL statement, kita gunakan fungsi `sql(sqlstatement)` pada spark session."
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select count(*) from app_record\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select * from app_record limit 5\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Kita akan melakukan join salah satu kolom dengan data referensi dan melakukan agregasi. Sebelumnya kita buat dataframe referensi dan membuat temporary viewnya\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmydata \u003d (\n    (\u0027Lower secondary\u0027,1),\n    (\u0027Secondary / secondary special\u0027,2),\n    (\u0027Academic degree\u0027,3),\n    (\u0027Incomplete higher\u0027,4),\n    (\u0027Higher education\u0027,5))\n\nref_edu \u003d spark.createDataFrame(mydata).toDF(\"NAME_EDUCATION_TYPE\", \"EDU_LEVEL\")\nref_edu.createOrReplaceTempView(\"ref_edu\")\nspark.sql(\"select * from ref_edu\").show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Kita lakukan join, agregat, kemudian kita simpan ke tabel"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"SELECT edu_level, count(1) as number_of_app FROM\n              (SELECT ref_edu.EDU_LEVEL as edu_level\n                FROM app_record LEFT JOIN ref_edu\n                ON app_record.NAME_EDUCATION_TYPE\u003dref_edu.NAME_EDUCATION_TYPE)\n             GROUP BY edu_level SORT BY edu_level\"\"\").write.saveAsTable(name\u003d\"aggregated_edu\", mode\u003d\"overwrite\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Kita tampilkan hasilnya dengan menggunakan perintah `describe formatted`\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"describe formatted aggregated_edu\").show(truncate \u003d False)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /warehouse/tablespace/managed/hive/aggregated_edu\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "##### Delete All Data before running first time\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -rmr -skipTrash /user/userdev/csv_dataframe"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n"
    }
  ]
}