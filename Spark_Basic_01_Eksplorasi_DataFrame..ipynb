{
  "metadata": {
    "name": "Spark_Basic_01_Eksplorasi_DataFrame",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Membuat Spark session"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Import library yang akan digunakan."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Inisialisasi spark session untuk berinteraksi dengan Spark cluster"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark \u003d SparkSession.builder.appName(\u0027DataFrame Basics\u0027).getOrCreate()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Membuat DataFrame"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "DataFrame dapat dibuat dengan banyak cara, di antaranya :\n- Dari python object, misalnya array/list, dictionary, pandas dataframe, dll\n- Dari file : csv, json, dll\n- Dari HDFS\n- Dari RDD\n- dll."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1.1 Create From Array"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmydata \u003d ((\u0027DKI JAKARTA\u0027,15328),\n(\u0027JAWA BARAT\u0027,1320),\n(\u0027JAWA TENGAH\u0027,1030),\n(\u0027DI YOGYAKARTA\u0027,1174),\n(\u0027JAWA TIMUR\u0027,813),\n(\u0027BANTEN\u0027,1237))\n\ndf_from_array \u003d spark.createDataFrame(mydata).toDF(\"province\", \"density\")\n\ndf_from_array.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1.2 Create from Pandas DataFrame"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Download File yang akan di gunakan"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/penduduk2015.csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Create pandas dataframe dari file csv tersebut"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport pandas as pd\n\npddf \u003d pd.read_csv(\u0027/home/userdev/penduduk2015.csv\u0027)\npddf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Ubah ke Spark dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_from_pandas \u003d spark.createDataFrame(pddf)\ndf_from_pandas.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1.3. Create from csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Kita juga bisa me-load langsung file csv tersebut ke Spark dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -mkdir /user/userdev/csv_data\nhdfs dfs -put /home/userdev/penduduk2015.csv /user/userdev/csv_data"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_from_csv \u003d spark.read.csv(\"/user/userdev/csv_data/penduduk2015.csv\", header\u003dTrue)\ndf_from_csv.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1.4. Create from JSON"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Download File yang akan di gunakan\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/penduduk2015.json"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Tampilkan isi file dengan perintah `cat`"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\ncat penduduk2015.json"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk membaca multiline JSON, set parameter `multiline \u003d True`"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -mkdir /user/userdev/json_data\nhdfs dfs -put /home/userdev/penduduk2015.json /user/userdev/json_data"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfj \u003d spark.read.json(\"/user/userdev/json_data/penduduk2015.json\", multiLine\u003dTrue)\ndfj.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Explorasi DataFrame"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Dalam latihan ini kita akan mencoba berbagai operasi pada Spark DataFrame untuk melakukan eksplorasi data.\n\nKita akan menggunakan data kepadatan penduduk per propinsi."
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/indonesia2013-2015.csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Kita gunakan magic command untuk melihat ukuran dan isi file (karena file kita cukup kecil)."
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nls -al indonesia2013-2015.csv"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\ncat indonesia2013-2015.csv | head"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Karena data yang kita load sudah bersih, kita akan set `inferSchema \u003d True` agar Spark menyesuaikan tipe kolom dengan datanya."
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -put indonesia2013-2015.csv /user/userdev/csv_data"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d spark.read.csv(\"/user/userdev/csv_data/indonesia2013-2015.csv\",header\u003dTrue,inferSchema\u003dTrue)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.1 Melihat sekilas data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Menampilkan beberapa baris**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Biasanya kita menampilkan beberapa baris data untuk mengecek format dan konten dataframe yang kita buat.\n\nUntuk menampilkan beberapa baris dari dataframe, kita bisa gunakan perintah `show(n)` untuk menampilkan n baris pertama, atau `first()`` untuk menampilkan 1 baris pertama saja."
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.show(5)\ndf.first()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Menampilkan jumlah kolom**"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.columns"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Menampilkan total records**"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Menampilkan skema**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk menampilkan skema dataframe, gunakan fungsi `printSchema()`"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Akses atribut `columns` untuk menampilkan list nama kolom"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.columns"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Akses atribut `dtypes` untuk menampilkan list nama kolom beserta data type masing-masing kolom tersebut"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.dtypes"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Menampilkan summary statistik**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Fungsi `describe()` digunakan untuk menampilkan summary statistik dari seluruh kolom.\n\nJangan lupa memanggil fungsi `show()` untuk menampilkan hasilnya."
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.describe().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk menampilkan statistik dari salah satu kolom saja, gunakan nama kolom yang akan ditampilkan sebagai parameter. Misalnya `describe(\u0027column1\u0027)`"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.describe(\"density\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.2 Filtering data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Kita dapat melakukan filtering terhadap spark dataframe, berdasar kolom atau baris"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Memilih kolom tertentu**\n\nUntuk menampilkan kolom tertentu, digunakan fungsi `select(\u0027nama_kolom\u0027)`"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.select(\"year\").show(10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk memilih beberapa kolom, gunakan tanda koma sebagai pemisah"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.select(\"province\",\"density\").show(10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Memilih records / baris**\n\nUntuk memilih baris dengan kondisi tertentu, gunakan fungsi `filter(\u003ckondisi\u003e)`"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.filter(df.density \u003e 5000).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk menggunakan kondisi berupa operasi string, dapat digunakan fungsi-fungsi dari `pyspark.sql.Column` yang terkait string, misalnya `contains(), startswith(), endswith()`"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.filter(df.province.contains(\u0027TENGGARA\u0027)).show(5)\ndf.filter(df.province.startswith(\u0027SU\u0027)).show(5)\ndf.filter(df.province.endswith(\u0027BARAT\u0027)).show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Tersedia juga fungsi `like()`` yang serupa dengan SQL statement like"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.filter(df.province.like(\u0027SU%\u0027)).show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Atau dapat juga menggunakan regex, dengan fungsi `rlike()`"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.filter(df.province.rlike(\u0027[A-Z]*TA$\u0027)).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Dapat juga menggunakan filter berdasar list, dengan fungsi `isin()`"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.filter(df.timezone.isin(\u0027WIT\u0027,\u0027WITA\u0027)).show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk menggunakan beberapa kondisi sekaligus, menggunakan tanda `\u0026` untuk *AND* dan `|` untuk *OR*, dengan masing-masing kondisi dilingkupi tanda kurung `()`"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.filter((df.timezone.isin(\u0027WIT\u0027,\u0027WITA\u0027)) \u0026 (df.year \u003d\u003d 2013)).show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.3 Unique value"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Menampilkan nilai unik**\n\nUntuk menampilkan nilai unik dari dataframe, digunakan fungsi `distinct()`.\n\nNilai unik di sini adalah kombinasi nilai dari seluruh kolom."
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.distinct().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk menampilkan nilai unik dari kolom tertentu, tulis nama kolom yang dimaksud sebagai parameter."
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.select(\u0027timezone\u0027).distinct().show()\ndf.select(\u0027year\u0027,\u0027timezone\u0027).distinct().show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Menghapus duplikasi data**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk menghapus record duplikat, gunakan `dropDuplicates(subset)` atau `drop_duplicates(subset)`."
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.dropDuplicates([\u0027province\u0027, \u0027timezone\u0027]).show(100)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.4 Agregasi"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Group by column**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk mengelompokkan berdasar kolom, gunakan perintah `groupBy(\u0027nama_kolom\u0027)`\n\nUntuk mengelompokkan berdasar lebih dari 1 kolom, gunakan tanda koma sebagai pemisah nama kolom."
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.groupBy(\"timezone\")"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.groupBy(\"timezone\",\"year\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Perintah `groupBy()` menghasilkan obyek `GroupedData` yang belum bisa ditampilkan.\n\nBiasanya setelah pengelompokan, kita melakukan operasi sumarisasi data. Kita terapkan operasi tersebut pada objek hasil groupBy dengan memanggil fungsi yang dibutuhkan. Misalnya `count()` atau `max(\u0027namakolom\u0027)`"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.groupBy(\u0027timezone\u0027).count().show()\ndf.groupBy(\u0027timezone\u0027).max(\u0027density\u0027).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Kita juga bisa menggunakan fungsi `agg()` untuk melakukan agregasi. Terutama jika kita ingin melakukan lebih dari 1 operasi agregat.\n\nKita bisa menggunakan fungsi `alias()` untuk memberi nama kolom hasil agregasi."
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.groupBy(\"timezone\").agg(F.max(\u0027density\u0027)).show()\ndf.groupBy(\"timezone\").agg(F.avg(\u0027density\u0027).alias(\u0027avg_density\u0027), \\\n                           F.min(\u0027density\u0027).alias(\u0027min_density\u0027), \\\n                           F.max(\u0027density\u0027).alias(\u0027max_density\u0027)).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Order By**"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.groupBy(\"timezone\") \\\n  .mean(\"density\") \\\n  .orderBy(\"timezone\",ascending\u003dFalse).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Agregasi dengan filter / kondisi**"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.groupBy(\"timezone\") \\\n  .mean(\"density\") \\\n  .where(df.timezone.contains(\u0027WIT\u0027)).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Filter hasil agregat `(SQL stat HAVING)`**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk memfilter berdasar hasil agregasi (semacam perintah `HAVING` di SQL), lakukan dalam 2 langkah.\n\n1. Lakukan `groupBy` + `agg` dan beri nama kolom hasil agregat dengan `alias`\n2. gunakan fungsi `filter(kondisi)` pada kolom hasil agregasi."
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_agg \u003d df.groupBy(\"timezone\", \"province\") \\\n  .agg(F.avg(\"density\").alias(\"avg_density\")) \\\n  .where(df.timezone.contains(\u0027WIT\u0027))\n\ndf_agg.filter(df_agg.avg_density \u003e 50).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.5 Transformasi DataFrame"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Kolom baru berupa nilai konstan**\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk menambahkan kolom baru ke dalam dataframe, kita bisa menggunakan perintah `withColumn()`\n\nSedangkan untuk menambahkan sebuah nilai konstan, kita bisa menggunakan fungsi `lit(nilai_konstan)` dari `pyspark.sql.functions`, yang berfungsi membuat kolom dari nilai literal/konstan.\n\nMisalnya kita ingin menambahkan kolom status yang bernilai 1"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.withColumn(\u0027status\u0027, F.lit(1)).show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Kolom baru dari kolom yang ada**"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.withColumn(\u0027tahun-1\u0027, df.year-1).show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Kondisional**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk menambahkan kolom berdasar beberapa kondisi, gunakan `when` dan `otherwise` (jika perlu).\n\nPerhatikan bahwa `when` yang pertama adalah fungsi dalam `pyspark.sql.functions`, sedangkan `when` yang berikutnya adalah fungsi `when` pada object kolom `(pyspark.sql.Column)`\n\nFungsi `otherwise` adalah kondisi else atau kondisi selain yang disebutkan pada when."
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.withColumn(\u0027timezone1\u0027, F.when(df.timezone \u003d\u003d \u0027WIT\u0027, 1).\n              when(df.timezone \u003d\u003d \u0027WIT\u0027, 2).\n              otherwise(3)).show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.6 Data enrichment - Join"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Perintah ntuk melakukan join adalah sebagai berikut :\n\n`df1.join(df2, on\u003d[columname], how\u003d’left’)``\n\nWhere :\n\n- `df1` − Dataframe1.\n- `df2` – Dataframe2.\n- `on` − nama kolom yang akan digunakan untuk join.\n- `how` – type of join needs to be performed – left, right, outer, inner. Defaultnya adalah inner join.\n\nCreate dataframe yang akan digunakan untuk join"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndamdata \u003d ((\"SUMATERA SELATAN\",2),\n(\"SULAWESI TENGAH\",2),\n(\"SULAWESI SELATAN\",2),\n(\"SUMATERA BARAT\",3),\n(\"RIAU\",3),\n(\"LAMPUNG\",3),\n(\"NUSA TENGGARA TIMUR\",4),\n(\"BENGKULU\",8),\n(\"SUMATERA UTARA\",10),\n(\"JAWA TIMUR\",12),\n(\"JAWA TENGAH\",35),\n(\"JAWA BARAT\",49))\n\ndf_dam \u003d spark.createDataFrame(damdata).toDF(\"province\", \"dam_num\")\n\ndf_dam.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Join dataframe kepadatan penduduk dengan dataframe jumlah bendungan, berdasarkan nama propinsi."
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.join(df_dam, on\u003d[\u0027province\u0027], how\u003d\u0027inner\u0027).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Untuk melakukan left join, tentukan parameter `how`"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.join(df_dam, on\u003d[\u0027province\u0027], how\u003d\u0027left\u0027).show(40)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "##### Delete all data before running first time\n"
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -rm -r -skipTrash /user/userdev/csv_data\nhdfs dfs -rm -r -skipTrash /user/userdev/json_data\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n"
    }
  ]
}