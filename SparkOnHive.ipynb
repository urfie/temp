{
{ "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### **Import package yang akan kita gunakan**\n#### Untuk melakukan koneksi ke Hive, kita perlu menjalankan fungsi `enableHiveSupport()` pada saat membuat spark session"]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": ["%spark.pyspark\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder.appName(\u0027Hive Basics\u0027).enableHiveSupport().getOrCreate()\n"]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": ["%spark.pyspark\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder.enableHiveSupport().getOrCreate()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### **Menjalankan perintah SHOW dan DESCRIBE**\n"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["#### Untuk menjalankan SQL command ke dalam Hive, kita gunakan fungsi `spark.sql()`. Fungsi ini mengembalikan spark DataFrame, sehingga untuk menampilkannya kita perlu memanggil fungsi `show()`"]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": ["%spark.pyspark\nspark.sql(\"show databases\").show()"]
    }
 ]
}
