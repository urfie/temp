{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e880ebf7",
   "metadata": {},
   "source": [
    "Import package yang akan kita gunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93ac9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bed7b",
   "metadata": {},
   "source": [
    "Untuk melakukan koneksi ke Hive, kita perlu menjalankan fungsi enableHiveSupport() pada saat membuat spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5337ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/02 13:34:06 WARN Utils: Your hostname, dl247-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.15.130 instead (on interface ens33)\n",
      "23/10/02 13:34:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/02 13:34:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Hive Basics').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fed27",
   "metadata": {},
   "source": [
    "## Menjalankan perintah SHOW dan DESCRIBE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d991c9a",
   "metadata": {},
   "source": [
    "Untuk menjalankan SQL command ke dalam Hive, kita gunakan fungsi `spark.sql()`. Fungsi ini mengembalikan spark DataFrame, sehingga untuk menampilkannya kita perlu memanggil fungsi `show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687d2f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   mytest|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314c430",
   "metadata": {},
   "source": [
    "Kita akan menggunakan database \"mytest\" untuk latihan ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97a94a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|   mytest|          emp|      false|\n",
      "|   mytest|      emp_ext|      false|\n",
      "|   mytest|  emp_landing|      false|\n",
      "|   mytest|      emp_orc|      false|\n",
      "|   mytest|     employee|      false|\n",
      "|   mytest|    employee1|      false|\n",
      "|   mytest| employee_ext|      false|\n",
      "|   mytest|    mahasiswa|      false|\n",
      "|   mytest|mahasiswa_ext|      false|\n",
      "|   mytest|          mhs|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables from mytest\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43d4faad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|        firstname|   string|   null|\n",
      "|         lastname|   string|   null|\n",
      "|            email|   string|   null|\n",
      "|           gender|   string|   null|\n",
      "|              age|      int|   null|\n",
      "|         jobtitle|   string|   null|\n",
      "|yearsofexperience|   bigint|   null|\n",
      "|           salary|      int|   null|\n",
      "|       department|   string|   null|\n",
      "|        datestamp|     date|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe mytest.employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e0b1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|firstname                   |string                      |null   |\n",
      "|lastname                    |string                      |null   |\n",
      "|email                       |string                      |null   |\n",
      "|gender                      |string                      |null   |\n",
      "|age                         |int                         |null   |\n",
      "|jobtitle                    |string                      |null   |\n",
      "|yearsofexperience           |bigint                      |null   |\n",
      "|salary                      |int                         |null   |\n",
      "|department                  |string                      |null   |\n",
      "|datestamp                   |date                        |null   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Database                    |mytest                      |       |\n",
      "|Table                       |employee                    |       |\n",
      "|Owner                       |hadoop                      |       |\n",
      "|Created Time                |Mon Oct 02 07:27:08 WIB 2023|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "|Created By                  |Spark 3.3.1                 |       |\n",
      "|Type                        |MANAGED                     |       |\n",
      "|Provider                    |hive                        |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted mytest.employee\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb8069df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|firstname                   |string                      |null   |\n",
      "|lastname                    |string                      |null   |\n",
      "|email                       |string                      |null   |\n",
      "|gender                      |string                      |null   |\n",
      "|age                         |int                         |null   |\n",
      "|jobtitle                    |string                      |null   |\n",
      "|yearsofexperience           |bigint                      |null   |\n",
      "|salary                      |int                         |null   |\n",
      "|department                  |string                      |null   |\n",
      "|datestamp                   |date                        |null   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Database                    |mytest                      |       |\n",
      "|Table                       |employee_ext                |       |\n",
      "|Owner                       |hadoop                      |       |\n",
      "|Created Time                |Mon Oct 02 07:30:49 WIB 2023|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "|Created By                  |Spark 3.3.1                 |       |\n",
      "|Type                        |EXTERNAL                    |       |\n",
      "|Provider                    |hive                        |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted mytest.employee_ext\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17eee8",
   "metadata": {},
   "source": [
    "## Melakukan query ke tabel Hive \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48d05d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+---+--------------------+-----------------+------+----------+----------+\n",
      "|firstname|lastname|               email|gender|age|            jobtitle|yearsofexperience|salary|department| datestamp|\n",
      "+---------+--------+--------------------+------+---+--------------------+-----------------+------+----------+----------+\n",
      "|     Jose|   Lopez|joselopez0944@sli...|  male| 25|     Project Manager|                1|  8500|   Product|2023-09-01|\n",
      "|    Diane|  Carter|dianecarter1228@s...|female| 26|Machine Learning ...|                2|  7000|   Product|2023-09-01|\n",
      "|    Shawn|  Foster|shawnfoster2695@s...|  male| 37|     Project Manager|               14| 17000|   Product|2023-09-01|\n",
      "|   Brenda|  Fisher|brendafisher3185@...|female| 31|       Web Developer|                8| 10000|   Product|2023-09-01|\n",
      "|     Sean|  Hunter|seanhunter4753@sl...|  male| 35|     Project Manager|               11| 14500|   Product|2023-09-01|\n",
      "+---------+--------+--------------------+------+---+--------------------+-----------------+------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM mytest.employee limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1587e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------+\n",
      "|count(1)|gender|    department|\n",
      "+--------+------+--------------+\n",
      "|       1|female|       product|\n",
      "|     159|  male|       Product|\n",
      "|     143|female|       Product|\n",
      "|       1|female|human resource|\n",
      "|       5|  male|Human Resource|\n",
      "|       2|     M|       Product|\n",
      "|       2|  male|       product|\n",
      "|       5|female|Human Resource|\n",
      "|       2|     F|       Product|\n",
      "+--------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(*), gender, department FROM mytest.employee \n",
    "            GROUP BY gender, department\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ae07e",
   "metadata": {},
   "source": [
    "## Membuat managed tabel dari dataframe\n",
    "\n",
    "Kita bisa membuat tabel dari sebuah dataframe. Untuk itu kita buat dataframenya terlebih dahulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2349a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+------+------+\n",
      "| nama|kode_jurusan|nilai1|nilai2|nilai3|\n",
      "+-----+------------+------+------+------+\n",
      "| Agus|           F|   100|   150|   150|\n",
      "|Windy|           F|   200|   150|   180|\n",
      "| Budi|           B|   200|   100|   150|\n",
      "| Dina|           F|   150|   150|   130|\n",
      "| Bayu|           F|    50|   150|   100|\n",
      "| Dedi|           B|    50|   100|   100|\n",
      "+-----+------------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [['Agus','F',100,150,150],['Windy','F',200,150,180],\n",
    "        ['Budi','B',200,100,150],['Dina','F',150,150,130],\n",
    "        ['Bayu','F',50,150,100],['Dedi','B',50,100,100]]\n",
    "\n",
    "kolom = [\"nama\",\"kode_jurusan\",\"nilai1\",\"nilai2\",\"nilai3\"]\n",
    "df = spark.createDataFrame(data,kolom)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d478f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"drop table mytest.mahasiswa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c24b4",
   "metadata": {},
   "source": [
    "Untuk menyimpan sebuah dataframe menjadi tabel kita menggunakan perintah `DataFrameWriter.saveAsTable()` ada beberapa parameter yang bisa kita pilih, diantaranya yaitu **mode** yang menyediakan pilihan nilai berupa : *append, overwrite, ignore, error, errorifexists*\n",
    "\n",
    "Untuk contoh ini kita pilih mode *overwrite*, dan kita beri nama tabelnya *mahasiswa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf639632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode('overwrite') \\\n",
    "         .saveAsTable(\"mytest.mahasiswa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1724a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|   mytest|          emp|      false|\n",
      "|   mytest|      emp_ext|      false|\n",
      "|   mytest|  emp_landing|      false|\n",
      "|   mytest|      emp_orc|      false|\n",
      "|   mytest|     employee|      false|\n",
      "|   mytest|    employee1|      false|\n",
      "|   mytest| employee_ext|      false|\n",
      "|   mytest|    mahasiswa|      false|\n",
      "|   mytest|mahasiswa_ext|      false|\n",
      "|   mytest|          mhs|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables from mytest\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cf2ee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|nama                        |string                                                        |null   |\n",
      "|kode_jurusan                |string                                                        |null   |\n",
      "|nilai1                      |bigint                                                        |null   |\n",
      "|nilai2                      |bigint                                                        |null   |\n",
      "|nilai3                      |bigint                                                        |null   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Database                    |mytest                                                        |       |\n",
      "|Table                       |mahasiswa                                                     |       |\n",
      "|Owner                       |hadoop                                                        |       |\n",
      "|Created Time                |Mon Oct 02 13:40:11 WIB 2023                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.3.1                                                   |       |\n",
      "|Type                        |MANAGED                                                       |       |\n",
      "|Provider                    |parquet                                                       |       |\n",
      "|Statistics                  |3286 bytes                                                    |       |\n",
      "|Location                    |hdfs://127.0.0.1:9000/user/hive/warehouse/mytest.db/mahasiswa |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted mytest.mahasiswa\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f189209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+------+------+\n",
      "| nama|kode_jurusan|nilai1|nilai2|nilai3|\n",
      "+-----+------------+------+------+------+\n",
      "| Agus|           F|   100|   150|   150|\n",
      "|Windy|           F|   200|   150|   180|\n",
      "| Budi|           B|   200|   100|   150|\n",
      "| Dina|           F|   150|   150|   130|\n",
      "| Bayu|           F|    50|   150|   100|\n",
      "| Dedi|           B|    50|   100|   100|\n",
      "+-----+------------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from mytest.mahasiswa\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c59a4f",
   "metadata": {},
   "source": [
    "## Membuat External Tabel dari DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82708592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hadoop/mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/hadoop/mydata/mahasiswa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cc75d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite') \\\n",
    "        .option(\"path\", \"hdfs://127.0.0.1:9000/user/hadoop/mydata/mahasiswa\") \\\n",
    "        .saveAsTable(\"mytest.mahasiswa_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb303d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|nama                        |string                                                        |null   |\n",
      "|kode_jurusan                |string                                                        |null   |\n",
      "|nilai1                      |bigint                                                        |null   |\n",
      "|nilai2                      |bigint                                                        |null   |\n",
      "|nilai3                      |bigint                                                        |null   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Database                    |mytest                                                        |       |\n",
      "|Table                       |mahasiswa_ext                                                 |       |\n",
      "|Owner                       |hadoop                                                        |       |\n",
      "|Created Time                |Mon Oct 02 09:15:56 WIB 2023                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.3.1                                                   |       |\n",
      "|Type                        |EXTERNAL                                                      |       |\n",
      "|Provider                    |parquet                                                       |       |\n",
      "|Statistics                  |3286 bytes                                                    |       |\n",
      "|Location                    |hdfs://127.0.0.1:9000/user/hadoop/mydata/mahasiswa            |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended mytest.mahasiswa_ext\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7796baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+------+------+\n",
      "| nama|kode_jurusan|nilai1|nilai2|nilai3|\n",
      "+-----+------------+------+------+------+\n",
      "| Agus|           F|   100|   150|   150|\n",
      "|Windy|           F|   200|   150|   180|\n",
      "| Budi|           B|   200|   100|   150|\n",
      "| Dina|           F|   150|   150|   130|\n",
      "| Bayu|           F|    50|   150|   100|\n",
      "| Dedi|           B|    50|   100|   100|\n",
      "+-----+------------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM mytest.mahasiswa_ext\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68894f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   3 hadoop supergroup          0 2023-10-02 09:15 /user/hadoop/mydata/mahasiswa/_SUCCESS\r\n",
      "-rw-r--r--   3 hadoop supergroup       1645 2023-10-02 09:15 /user/hadoop/mydata/mahasiswa/part-00000-186a9e72-b888-4439-878b-bbd9298c293a-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 hadoop supergroup       1641 2023-10-02 09:15 /user/hadoop/mydata/mahasiswa/part-00001-186a9e72-b888-4439-878b-bbd9298c293a-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hadoop/mydata/mahasiswa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8af96",
   "metadata": {},
   "source": [
    "## Membuat Managed Tabel dengan CREATE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table mytest.emp\")\n",
    "spark.sql(\"drop table mytest.emp_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a901a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS mytest.emp(\n",
    "firstname STRING,\n",
    "lastname STRING,\n",
    "email STRING,\n",
    "gender STRING,\n",
    "age INT,\n",
    "jobtitle STRING,\n",
    "yearsofexperience BIGINT,\n",
    "salary INT,\n",
    "department STRING)\n",
    "STORED AS ORC;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d35b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended mytest.emp\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /home/hadoop/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hadoop/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from mytest.emp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd42e0",
   "metadata": {},
   "source": [
    "## Membuat External Table dengan CREATE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad379ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /home/hadoop/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04889ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P /home/hadoop/datasets https://github.com/urfie/temp/raw/main/emp.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /home/hadoop/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hadoop/mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/hadoop/mydata/emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a35239",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /home/hadoop/datasets/emp.csv /user/hadoop/mydata/emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373601f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE  EXTERNAL TABLE mytest.emp_ext(\n",
    "firstname STRING,\n",
    "lastname STRING,\n",
    "email STRING,\n",
    "gender STRING,\n",
    "age INT,\n",
    "jobtitle STRING,\n",
    "yearsofexperience BIGINT,\n",
    "salary INT,\n",
    "department STRING)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs://127.0.0.1:9000/user/hadoop/mydata/emp'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809314ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from mytest.emp_ext\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended mytest.emp_ext\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352efa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from mytest.emp_ext limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fb8f8",
   "metadata": {},
   "source": [
    "## Insert into Hive Table from External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73377e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO mytest.emp SELECT * FROM mytest.emp_ext;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d584c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from mytest.emp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e133eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from mytest.emp limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b33895",
   "metadata": {},
   "source": [
    "## Menjalankan fungsi Hive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb902484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----------------+\n",
      "|lower(firstname)|lower(lastname)|lower(department)|\n",
      "+----------------+---------------+-----------------+\n",
      "|            jose|          lopez|          product|\n",
      "|           diane|         carter|          product|\n",
      "|           shawn|         foster|          product|\n",
      "|          brenda|         fisher|          product|\n",
      "|            sean|         hunter|          product|\n",
      "+----------------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select lower(firstname), lower(lastname), lower(department) from mytest.emp limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55535aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
