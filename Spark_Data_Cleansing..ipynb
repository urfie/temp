{
  "metadata": {
    "name": "Spark_Data_Cleansing",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Dalam latihan ini kita akan melakukan data cleansing dan transform menggunakan data credit card approval. \n#### Data diambil dari sini : `https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction/data`\n\n#### Yang akan kita lakukan adalah sebagai berikut :\n\n*1. Load data*\n*2. Eksplorasi*\n*3. Menangani empty/NULL column : mengisi dengan nilai default*\n*4. Melakukan agregasi dan join data credit application dan data credit record yang sudah diagregasi*\n*5. Transformasi tanggal*\n\n#### Yang akan kita lakukan tentu bukan proses menghitung credit risk yang ideal, akan tetapi hanya untuk menunjukkan beberapa operasi yang biasa kita lakukan dalam data preprocessing, dilakukan dengan spark dan hive."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as f\nspark \u003d SparkSession.builder.enableHiveSupport().getOrCreate()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Prepare Data\n\n#### Download Dataset\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget  -P data https://github.com/urfie/SparkSQL-dengan-Hive/raw/main/datasets/application_record.csv.gz"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget -P data https://github.com/urfie/SparkSQL-dengan-Hive/raw/main/datasets/credit_record.csv.gz"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Upload Data ke HDFS\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -mkdir /user/userdev/cc_data"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -mkdir /user/userdev/cc_data/credit"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -put data/credit_record.csv.gz /user/userdev/cc_data/credit"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/userdev/cc_data/credit"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -mkdir /user/userdev/cc_data/app"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -put data/application_record.csv.gz /user/userdev/cc_data/app"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/userdev/cc_data/app"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"CREATE DATABASE user0;\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Create External table\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"CREATE EXTERNAL TABLE user0.credit_ext(\nid STRING,\nmonths_balance BIGINT,\nstatus STRING)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY \u0027,\u0027\nSTORED AS TEXTFILE\nLOCATION \u0027hdfs://myzoo/user/userdev/cc_data/credit\u0027\n\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select * from user0.credit_ext limit 10\").show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"\nCREATE EXTERNAL TABLE user0.app_ext(\nid\tSTRING,\ncode_gender\tSTRING,\nflag_own_car\tSTRING,\nflag_own_realty\tSTRING,\ncnt_children\tSTRING,\namt_income_total\tSTRING,\nname_income_type\tSTRING,\nname_education_type\tSTRING,\nname_family_status\tSTRING,\nname_housing_type\tSTRING,\ndays_birth\tINT,\ndays_employed\tINT,\nflag_mobil\tINT,\nflag_work_phone\tINT,\nflag_phone\tINT,\nflag_email\tINT,\noccupation_type\tSTRING,\ncnt_fam_members\tINT)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY \u0027,\u0027\nSTORED AS TEXTFILE\nLOCATION \u0027hdfs://myzoo/user/userdev/cc_data/app\u0027\n\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select * from user0.app_ext limit 5\").show(truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Create Managed Table\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"CREATE TABLE user0.credit(\nid STRING,\nmonths_balance BIGINT,\nstatus STRING)\nSTORED AS ORC;\n\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"insert into user0.credit select * from user0.credit_ext\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"\nCREATE TABLE user0.app(\nid\tSTRING,\ncode_gender\tSTRING,\nflag_own_car\tSTRING,\nflag_own_realty\tSTRING,\ncnt_children\tSTRING,\namt_income_total\tSTRING,\nname_income_type\tSTRING,\nname_education_type\tSTRING,\nname_family_status\tSTRING,\nname_housing_type\tSTRING,\ndays_birth\tINT,\ndays_employed\tINT,\nflag_mobil\tINT,\nflag_work_phone\tINT,\nflag_phone\tINT,\nflag_email\tINT,\noccupation_type\tSTRING,\ncnt_fam_members\tINT)\nSTORED AS ORC;\n\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"insert into user0.app select * from user0.app_ext\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Clean Data\n\n#### Explorasi Data"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct status from user0.credit_ext\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct name_education_type from user0.app_ext\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct code_gender from user0.app_ext\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct name_family_status from user0.app_ext\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct name_housing_type from user0.app_ext\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct occupation_type from user0.app_ext\").show(truncate \u003d False)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "##### Dari hasil di atas terlihat bahwa kolom `occupation_type` memiliki record dengan nilai kosong (empty string). Note : Dalam hal ini Hive menyimpan kolom kosong sebagai *empty string*, bukan *NULL*\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Load Data ke DataFrame"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfApp \u003d spark.sql(\"select * from user0.app_ext\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Drop Duplicate Row\n\n##### Membersihkan data duplikat, yaitu record dengan id yang sama\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfApp.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfNodup \u003d dfApp.drop_duplicates([\"id\"])"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfNodup.count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Mengisi Nilai Default\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfClean \u003d dfNodup.withColumn(\u0027occupation_type\u0027, \\\n                              f.when(f.trim(dfNodup[\u0027occupation_type\u0027]) \u003d\u003d \u0027\u0027, \u0027Other\u0027) \\\n                              .otherwise(dfNodup[\u0027occupation_type\u0027]))"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfClean.select(\u0027occupation_type\u0027).distinct().show(truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Reformat tanggal lahir\n##### Kita akan menghitung tanggal lahir dengan rumus : `tanggal saat ini - days_birth`\n"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfClean \u003d dfClean.withColumn(\u0027dob\u0027, f.expr(\"date_add(current_date(), days_birth)\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfClean.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Reformat status kredit\n\nBerdasar deskripsi dataset, status kredit terdiri dari C, X, dan integer 0-5, dengan arti :\n\n- 0: 1-29 days past due\n- 1: 30-59 days past due\n- 2: 60-89 days overdue\n- 3: 90-119 days overdue\n- 4: 120-149 days overdue\n- 5: Overdue or bad debts, write-offs for more than 150 days\n- C: paid off that month\n- X: No loan for the month\n- Kita akan mengelompokkan jenis kredit menjadi 2, yaitu\n\n0 (Good Credit): X, C, dan 0\n1 (Bad Credit): 2 sampai 5\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfCredit \u003d spark.sql(\"select * from user0.credit\")"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfCredit \u003d dfCredit.withColumn(\u0027status_new\u0027, \\\n                              f.when(dfCredit.status.isin([\u0027X\u0027,\u0027C\u0027,0]), 0) \\\n                              .otherwise(1))\n                              \ndfAggregated \u003d dfCredit.groupBy(\"id\").agg(f.sum(\"status_new\").alias(\"risk\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Join dengan data aplikasi\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfApp_joined \u003d dfClean.join(dfAggregated,dfClean.id\u003d\u003ddfAggregated.id, \"left\").select(dfClean[\"*\"],dfAggregated[\"risk\"])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Save ke tabel\n"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfApp_joined.write.saveAsTable(name\u003d\"user0.app_risk_1\", mode\u003d\"overwrite\",format\u003d\"orc\")"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"describe extended user0.app_risk_1\").show(100, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select * from user0.app_risk_1\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "##### Drop All data before running first\n"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"drop table user0.app\")\nspark.sql(\"drop table user0.app_ext\")\nspark.sql(\"drop table user0.credit\")\nspark.sql(\"drop table user0.credit_ext\")\nspark.sql(\"drop table user0.app_risk_1\")\nspark.sql(\"drop database user0\")"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -rmr -skipTrash /user/userdev/cc_data\n"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"\n    SELECT app.*, credit.resiko \n        FROM \n        (SELECT id,code_gender,flag_own_car,flag_own_realty,cnt_children,amt_income_total,    \n    name_income_type,name_education_type,name_family_status,name_housing_type,days_birth,\n    days_employed,flag_mobil,flag_work_phone,flag_phone,flag_email,\n              CASE WHEN trim(occupation_type) \u003d\u003d \u0027\u0027 THEN \u0027Other\u0027 ELSE occupation_type END as occupation_type,\n            cnt_fam_members, \n              date_add(current_date(), days_birth) as dob FROM \n                (SELECT distinct * FROM user0.app)) as app \n        JOIN\n        (SELECT id, sum(status_new) as resiko from \n            (SELECT id, status, CASE WHEN status in (\u0027C\u0027,\u0027X\u0027,\u00270\u0027) THEN 0 ELSE 1 END as status_new FROM user0.credit) \n        GROUP BY id) as credit\n        ON app.id \u003d credit.id\"\"\").write.saveAsTable(name\u003d\"user0.app_risk_2\", mode\u003d\"overwrite\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark.sql(\"select * from user0.app_risk_2 limit 5\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"\n    SELECT app.*, credit.resiko \n        FROM \n        (SELECT id,code_gender,flag_own_car,flag_own_realty,cnt_children,amt_income_total,    \n    name_income_type,name_education_type,name_family_status,name_housing_type,days_birth,\n    days_employed,flag_mobil,flag_work_phone,flag_phone,flag_email,\n              CASE WHEN trim(occupation_type) \u003d\u003d \u0027\u0027 THEN \u0027Other\u0027 ELSE occupation_type END as occupation_type,\n            cnt_fam_members, \n              date_add(current_date(), days_birth) as dob FROM \n                (SELECT distinct * FROM user0.app)) as app \n        JOIN\n        (SELECT id, sum(status_new) as resiko from \n            (SELECT id, status, CASE WHEN status in (\u0027C\u0027,\u0027X\u0027,\u00270\u0027) THEN 0 ELSE 1 END as status_new FROM user0.credit) \n        GROUP BY id) as credit\n        ON app.id \u003d credit.id\"\"\").write.saveAsTable(name\u003d\"user0.app_risk_3\", mode\u003d\"overwrite\")"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}