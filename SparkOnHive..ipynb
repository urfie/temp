{
  "metadata": {
    "name": "SparkOnHive",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Import package yang akan kita gunakan**\n#### Untuk melakukan koneksi ke Hive, kita perlu menjalankan fungsi `enableHiveSupport()` pada saat membuat spark session"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder.appName(\u0027Hive Basics\u0027).enableHiveSupport().getOrCreate()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder.enableHiveSupport().getOrCreate()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Menjalankan perintah SHOW dan DESCRIBE**\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Untuk menjalankan SQL command ke dalam Hive, kita gunakan fungsi `spark.sql()`. Fungsi ini mengembalikan spark DataFrame, sehingga untuk menampilkannya kita perlu memanggil fungsi `show()`"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"show databases\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"describe database default\").show(truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Menjalankan perintah CREATE DATABASE**"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"create database mytest;\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"describe database mytest\").show(truncate \u003d False)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Membuat managed tabel dari dataframe**\n\nKita bisa membuat tabel dari sebuah dataframe. Untuk itu kita buat dataframenya terlebih dahulu"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndata \u003d [[\u0027Agus\u0027,\u0027F\u0027,100,150,150],[\u0027Windy\u0027,\u0027F\u0027,200,150,180],\n        [\u0027Budi\u0027,\u0027B\u0027,200,100,150],[\u0027Dina\u0027,\u0027F\u0027,150,150,130],\n        [\u0027Bayu\u0027,\u0027F\u0027,50,150,100],[\u0027Dedi\u0027,\u0027B\u0027,50,100,100]]\n\nkolom \u003d [\"nama\",\"kode_jurusan\",\"nilai1\",\"nilai2\",\"nilai3\"]\ndf \u003d spark.createDataFrame(data,kolom)\ndf.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Untuk menyimpan sebuah dataframe menjadi tabel kita menggunakan perintah `DataFrameWriter.saveAsTable()` ada beberapa parameter yang bisa kita pilih, diantaranya yaitu **mode** yang menyediakan pilihan nilai berupa : *append, overwrite, ignore, error, errorifexists*\n\nUntuk contoh ini kita pilih mode *overwrite*, dan kita beri nama tabelnya *mahasiswa*"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.write.mode(\u0027overwrite\u0027).saveAsTable(\"mytest.mahasiswa\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"show tables from mytest\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Untuk menampilkan property lengkap dari sebuah tabel, kita gunakan opsi `formatted` atau `extended`"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"describe formatted mytest.mahasiswa\").show(truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Melakukan query ke tabel Hive**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select * from mytest.mahasiswa\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"SELECT  kode_jurusan, count(*) as jumlah_mhs, avg(nilai1) as rata2_nilai1 \n            FROM mytest.mahasiswa \n            GROUP BY kode_jurusan\"\"\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Membuat External Tabel dari DataFrame**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -mkdir /user/userdev/mydata\nhdfs dfs -mkdir /user/userdev/mydata/mahasiswa"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.write.mode(\u0027overwrite\u0027) \\\n        .option(\"path\", \"hdfs://myzoo/user/userdev/mydata/mahasiswa\") \\\n        .saveAsTable(\"mytest.mahasiswa_ext\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"describe extended mytest.mahasiswa_ext\").show(truncate\u003dFalse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"SELECT * FROM mytest.mahasiswa_ext\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/userdev/mydata/mahasiswa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Membuat External Table dengan CREATE TABLE**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Persiapan data untuk di load ke external table"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nwget https://github.com/urfie/SparkSQL-dengan-Hive/raw/main/datasets/emp_clean.csv\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -mkdir /user/userdev/mydata/emp"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -put emp_clean.csv /user/userdev/mydata/emp"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/userdev/mydata/emp"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Create External Table\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"CREATE  EXTERNAL TABLE mytest.emp_ext(\nfirstname STRING,\nlastname STRING,\nemail STRING,\ngender STRING,\nage INT,\njobtitle STRING,\nyearsofexperience BIGINT,\nsalary INT,\ndepartment STRING)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY \u0027,\u0027\nSTORED AS TEXTFILE\nLOCATION \u0027hdfs://myzoo/user/userdev/mydata/emp\u0027\n\"\"\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"describe extended mytest.emp_ext\").show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select count(*) from mytest.emp_ext\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select * from mytest.emp_ext limit 5\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Menjalankan fungsi Hive**"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select lower(firstname), lower(lastname), lower(department) from mytest.emp_ext limit 5\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### **Membuat Tabel dengan Partisi**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Kita akan gunakan kolom `department` sebagai partisinya"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS mytest.emp_part(\nfirstname STRING,\nlastname STRING,\nemail STRING,\ngender STRING,\nage INT,\njobtitle STRING,\nyearsofexperience BIGINT,\nsalary INT)\npartitioned by (department string)\nSTORED AS ORC\nLOCATION \u0027hdfs://myzoo/user/userdev/mydata/emp_part\u0027;\n\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"describe formatted mytest.emp_part\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Insert data dari tabel non partisi ke tabel dengan partisi"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"insert overwrite table mytest.emp_part partition(department) select *  from mytest.emp_ext;\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Tampilkan data dari tabel `emp_part`"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select * from mytest.emp_part\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Tampilkan lokasi fisik tabel di hdfs"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /apps/spark/warehouse/mytest.db"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /apps/spark/warehouse/mytest.db/mahasiswa"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/userdev/mydata/emp_part"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/userdev/mydata/emp_part/department\u003dProduct"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Delete All Data before running first time"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"drop table mytest.emp_ext;\")\nspark.sql(\"drop table mytest.emp_part;\")\nspark.sql(\"drop table mytest.mahasiswa_ext;\")\nspark.sql(\"drop table mytest.mahasiswa;\")"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"drop database mytest;\")"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/userdev/mydata\n"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -rmr -skipTrash /user/userdev/mydata"
    }
  ]
}